%=============================================================================
\subsection{Pattern \& Sequential Stack}
\label{sec:pattern-stack}

\textbf{Stack overview:} Detect and complete patterns from context. Enable in-context learning through pattern matching and repetition detection. Support sequence continuation and algorithmic behavior. Process positional information for order-aware computation.

\textbf{Note:} Two mechanisms from this stack have been relocated: Local Context Modeling and Repetition \& Cycle Recognition moved to Appendix A (may be components of Pattern Completion rather than independent mechanisms).

%-----------------------------------------------------------------------------
\subsubsection{Pattern Completion Mechanism}
\label{mech:pattern-completion}

\noindent\depthinfo{0.05--0.58 (E-M)} | \primaryimpl{Attention heads + MLP neurons} | \litnames{induction mechanism, pattern completion, in-context learning}

\begin{functiondesc}
Detect and complete patterns of form [A][B]...[A] $\rightarrow$ predict [B]. Enable in-context learning through multi-stage circuit combining position-based copying, repetition detection, pattern matching, and n-gram retrieval. Foundation for few-shot learning and analogical reasoning. Support pattern-based prediction without parameter updates. Implement through coordinated multi-component circuit spanning 3--8 layers.

\textbf{Repetition detection component:} Duplicate-token heads detect repeated elements at multiple scales (token-level, phrasal, cyclical patterns) and provide repetition signals for pattern matching, entity tracking, output routing, and disambiguation. This repetition detection serves multiple downstream mechanisms as an implementation detail of the broader pattern completion circuit.
\end{functiondesc}

\begin{implementationbox}
\attnimpl{Previous-token heads (E, 0.05--0.20) create shifted representations through uniform offset attention. Duplicate-token heads (M, 0.30--0.58) detect repeated elements via exact token matching and provide repetition signals. Induction heads (M, 0.25--0.55) match patterns by attending to previous occurrences of current token content.}\\
\mlpimpl{N-gram neurons (E-M, 0.15--0.55) store frequent bigram and trigram patterns as key-value associations. Provide statistical support for pattern completion.}\\
\circuitimpl{Previous-token $\rightarrow$ Induction $\rightarrow$ MLP retrieval working in composition across 3--8 layers. Unified pattern completion circuit integrating multiple specialized components. Duplicate-token heads serve pattern completion, entity tracking, and output routing (IOI circuit) through multi-purpose repetition detection.}
\end{implementationbox}

\begin{ablationbox}
\textbf{Expected ablation:} Severe loss of in-context learning capability. Major degradation on few-shot tasks requiring pattern-based generalization. Loss of pattern completion and analogical continuation. Reduced ability to learn from examples provided in prompt without fine-tuning. Moderate impact on entity tracking and IOI circuit performance due to loss of repetition signals.
\end{ablationbox}

\begin{examplebox}
\exinput{``When Mary and John went to the store, Mary gave milk to John. When Susan and Bob went to the store, Susan gave milk to...''}\\
\exbehavior{Detect pattern [A] gave milk to [B], previous-token creates shifted representations, induction heads match pattern, duplicate-token tracks repetition}\\
\exeffect{Output ``Bob'' by analogical pattern completion across contexts}

\vspace{0.3em}
\textbf{Repetition detection:} ``The cat climbed the tree. The cat...'' $\rightarrow$ Second ``cat'' detected as duplicate, enabling entity linking and pattern completion.
\end{examplebox}

\mechfooter{\statuswell}{algorithmic-continuation, entity-grounding, output-routing}

%-----------------------------------------------------------------------------
\subsubsection{Algorithmic Continuation Mechanism}
\label{mech:algorithmic-continuation}

\noindent\depthinfo{0.35--0.65 (M)} | \primaryimpl{Attention heads + MLP neurons} | \litnames{sequence continuation, rule following, mathematical pattern completion}

\begin{functiondesc}
Continue algorithmic sequences by detecting underlying rules: counting, arithmetic progressions, alphabetic sequences, mathematical patterns. Extract systematic rules from examples and apply consistently. Support rule-based generation beyond memorization. Enable mathematical and logical pattern completion. Integration point between pattern matching and reasoning mechanisms. Implement structured sequence understanding.
\end{functiondesc}

\begin{implementationbox}
\attnimpl{Algorithmic heads (M, 0.35--0.60) detect regular patterns and systematic relationships in sequences through content-based attention to sequence structure.}\\
\mlpimpl{Rule storage neurons (M, 0.40--0.65) encode mathematical operations and sequence transformation rules. Store procedural knowledge for systematic continuation.}\\
\circuitimpl{Integrates with reasoning mechanisms (M-L) for complex rule application. Bridges pattern detection and symbolic reasoning.}
\end{implementationbox}

\begin{ablationbox}
\textbf{Expected ablation:} Significant loss of algorithmic continuation ability. Major degradation on sequence completion tasks requiring rule extraction. Reduced performance on mathematical and logical patterns. Loss of systematic rule application. Increased reliance on memorized sequences rather than rule understanding.
\end{ablationbox}

\begin{examplebox}
\exinput{``2, 4, 8, 16, 32, ...''}\\
\exbehavior{Detect doubling pattern through ratio analysis, activate multiplication rule, apply systematically}\\
\exeffect{Output ``64'' as next power of 2 through rule-based continuation}
\end{examplebox}

\mechfooter{\statusobs}{pattern-completion, multi-step-reasoning}

\begin{openquestionsbox}
\textbf{Mechanistic characterization needed:} While behavioral evidence is strong, detailed circuit-level tracing of rule extraction and application mechanisms remains incomplete. Future research should identify specific neuron populations encoding different rule types and trace information flow from pattern detection to rule application.
\end{openquestionsbox}

%-----------------------------------------------------------------------------
\subsubsection{Position-Based Processing Mechanism}
\label{mech:position-based-processing}

\noindent\depthinfo{0.05--0.65 (E-M)} | \primaryimpl{Architecture + Attention heads} | \litnames{positional encoding, position representation, order information}

\begin{functiondesc}
Encode and process positional information (absolute and relative) to enable order-aware computation. Provide transformers with sequence order information absent from raw token embeddings. Distinguish token order and maintain structural position awareness. Implement various positional schemes: absolute positions, relative positions, learned encodings. Enable position-dependent patterns and sequential reasoning. Foundation for all order-aware computation.
\end{functiondesc}

\begin{implementationbox}
\archimpl{Positional encodings added to input embeddings. Various implementations: sinusoidal (absolute), learned (absolute), rotary (relative). Architecture-level provision of position information.}\\
\attnimpl{Positional heads (E, 0.05--0.20) process absolute position information for early-layer position-aware patterns. Relative-position heads (M, 0.35--0.65) compute position relationships and distance-dependent attention.}\\
\circuitimpl{Spans input encoding through middle layers. Absolute position (E) provides foundation, relative position (M) enables structural understanding.}
\end{implementationbox}

\begin{ablationbox}
\textbf{Expected ablation:} Severe loss of order-awareness. Model treats sequences as bags of words with position-invariant processing. Major degradation on tasks requiring sequential understanding. Loss of position-dependent patterns. Inability to distinguish ``Alice followed Bob'' from ``Bob followed Alice''. Critical failure of order-sensitive computation.
\end{ablationbox}

\begin{examplebox}
\exinput{``Alice followed Bob'' vs. ``Bob followed Alice''}\\
\exbehavior{Use positional encodings to distinguish subject position from object position based on token order}\\
\exeffect{Understand opposite meanings despite identical token sets through position information}
\end{examplebox}

\mechfooter{\statuswell}{pattern-completion}

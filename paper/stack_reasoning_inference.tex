%=============================================================================
\subsection{Reasoning \& Inference Stack}
\label{sec:reasoning-stack}

\textbf{Stack overview:} Enable multi-step reasoning, logical inference, and problem-solving. Support chain-of-thought generation, consistency checking, and strategic planning. Bridge pattern matching with symbolic reasoning.

%-----------------------------------------------------------------------------
\subsubsection{Multi-Step Reasoning Mechanism}
\label{mech:multi-step-reasoning}

\noindent\depthinfo{0.50--0.85 (M-L)} | \primaryimpl{MLP neurons + Attention heads} | \litnames{chain-of-thought, sequential reasoning, step-by-step processing}

\begin{functiondesc}
Perform multi-step logical reasoning by maintaining intermediate conclusions and building toward final answer. Support chain-of-thought generation: explicit intermediate steps improve accuracy. Enable decomposition of complex problems into manageable subproblems. Maintain reasoning state across generation steps. Integrate information from multiple reasoning chains. Support self-correction and backtracking when contradictions detected. Foundation for complex problem-solving requiring multiple inference steps.
\end{functiondesc}

\begin{implementationbox}
\mlpimpl{Reasoning neurons (M-L, 0.50--0.80) encode reasoning heuristics, logical rules, and inference patterns. Store common reasoning templates and problem-solving strategies.}\\
\attnimpl{Reasoning-integration heads (L, 0.65--0.85) attend to previous reasoning steps and intermediate conclusions to maintain coherent reasoning chains.}\\
\circuitimpl{Problem decomposition (M) $\rightarrow$ step generation (M-L) $\rightarrow$ integration (L) $\rightarrow$ conclusion formation (L-F). Iterative refinement across multiple generation steps.}
\end{implementationbox}

\begin{ablationbox}
\textbf{Expected ablation:} Severe degradation in complex reasoning tasks. Loss of multi-step inference capability. Major accuracy drop on problems requiring intermediate steps. Increased tendency toward direct but incorrect answers. Reduced benefit from chain-of-thought prompting. Difficulty maintaining reasoning coherence across steps.
\end{ablationbox}

\begin{examplebox}
\exinput{``If all A are B, and all B are C, what can we conclude about A and C?''}\\
\exbehavior{Generate intermediate step: ``A must be B'', then ``B must be C'', apply transitivity rule}\\
\exeffect{Output ``All A are C'' through explicit multi-step reasoning chain}
\end{examplebox}

\mechfooter{\statusobs}{planning-strategy, consistency-checking, factual-recall}

%-----------------------------------------------------------------------------
\subsubsection{Planning \& Strategy Selection Mechanism}
\label{mech:planning-strategy}

\noindent\depthinfo{0.60--0.85 (L)} | \primaryimpl{Attention heads + MLP neurons} | \litnames{strategic planning, approach selection, method determination}

\begin{functiondesc}
Select appropriate problem-solving strategies and plan solution approaches. Recognize problem types and activate relevant solution templates. Decide between strategies: analytical vs. intuitive, direct vs. decomposition, forward vs. backward reasoning. Maintain high-level solution plan while generating detailed steps. Enable metacognitive awareness of solution approach. Support strategy switching when initial approach fails. Meta-level control of reasoning process.
\end{functiondesc}

\begin{implementationbox}
\attnimpl{Strategy-selection heads (L, 0.65--0.80) recognize problem characteristics and bias toward appropriate approaches. Attend to problem indicators and task requirements.}\\
\mlpimpl{Strategy-encoding neurons (L, 0.60--0.85) store solution templates and problem-solving heuristics for different domains. Encode strategy-specific processing patterns.}\\
\circuitimpl{Problem classification (L) $\rightarrow$ strategy selection (L) $\rightarrow$ plan execution (L-F) $\rightarrow$ monitoring. Meta-level control of reasoning process.}
\end{implementationbox}

\begin{ablationbox}
\textbf{Expected ablation:} Moderate loss of strategic problem-solving. Suboptimal approach selection for problem types. Notable degradation on problems requiring specific methods. Reduced adaptability when strategies need adjustment. More random or default strategy application. Less efficient problem-solving paths.
\end{ablationbox}

\begin{examplebox}
\exinput{``Optimize this function'' vs. ``Prove this theorem''}\\
\exbehavior{Route first to numerical/calculus approach with gradient methods, second to logical/proof approach with deduction}\\
\exeffect{Apply appropriate methodology for each problem type}
\end{examplebox}

\mechfooter{\statusobs}{multi-step-reasoning, task-routing}

%-----------------------------------------------------------------------------
\subsubsection{Consistency Checking Mechanism}
\label{mech:consistency-checking}

\noindent\depthinfo{0.70--0.88 (L)} | \primaryimpl{Attention heads} | \litnames{verification, coherence checking, contradiction detection}

\begin{functiondesc}
Detect inconsistencies, contradictions, and logical errors in generated content. Compare current generation against previous statements for coherence. Check factual claims against retrieved knowledge. Verify logical validity of reasoning steps. Enable self-correction before final output. Support accuracy improvement through internal verification. Identify when revision or qualification needed. Final verification stage before content commitment.
\end{functiondesc}

\begin{implementationbox}
\attnimpl{Consistency-checking heads (L, 0.70--0.85) attend to potentially contradictory previous content and flag inconsistencies. Compare current generation with prior statements for logical coherence.}\\
\circuitimpl{Generation (L) $\rightarrow$ consistency check (L) $\rightarrow$ correction/continuation. May trigger regeneration or qualification. Works in late layers before final output commitment.}
\end{implementationbox}

\begin{ablationbox}
\textbf{Expected ablation:} Moderate increase in self-contradictions and logical errors. Reduced internal verification capability. Notable degradation in accuracy on multi-step problems requiring consistency. More frequent generation of mutually inconsistent statements. Loss of self-correction capability. Reduced reliability of complex reasoning.
\end{ablationbox}

\begin{examplebox}
\exinput{[Generated: ``X is larger than Y'' earlier, now generating about Y and X]}\\
\exbehavior{Check compatibility with previous statement before asserting ``Y exceeds X''}\\
\exeffect{Avoid direct contradiction or add necessary qualification to maintain consistency}
\end{examplebox}

\mechfooter{\statusobs}{multi-step-reasoning, factual-recall}

%-----------------------------------------------------------------------------
\subsubsection{Analogical Mapping Mechanism}
\label{mech:analogical-mapping}

\noindent\depthinfo{0.45--0.75 (M-L)} | \primaryimpl{Attention heads + MLP neurons} | \litnames{analogy formation, transfer, mapping}

\begin{functiondesc}
Recognize structural similarities between different domains and transfer solutions. Map concepts from familiar domain to novel domain. Identify deep analogies beyond surface similarity. Enable transfer learning within context. Support metaphorical understanding and explanation. Build correspondences between source and target domains. Generalize solutions from examples to new cases. Bridge pattern matching with abstract reasoning.
\end{functiondesc}

\begin{implementationbox}
\attnimpl{Analogy-detection heads (M-L, 0.45--0.70) identify structural parallels between different contexts. Attend to relational patterns rather than surface features.}\\
\mlpimpl{Abstraction neurons (M-L, 0.50--0.75) encode domain-general patterns enabling transfer. Store analogical mappings and structural correspondences.}\\
\circuitimpl{Pattern abstraction (M) $\rightarrow$ similarity detection (M-L) $\rightarrow$ transfer (L). Connects induction mechanism to reasoning through abstraction.}
\end{implementationbox}

\begin{ablationbox}
\textbf{Expected ablation:} Notable loss of analogical reasoning and transfer capability. Reduced ability to apply solutions from one domain to another. Degradation in understanding metaphors and analogies. More literal, less flexible problem solving. Difficulty with cross-domain reasoning. Impaired transfer learning.
\end{ablationbox}

\begin{examplebox}
\exinput{``Atoms are to molecules as [blank] are to words''}\\
\exbehavior{Detect structural analogy (composition relationship), abstract from chemistry to linguistics domain}\\
\exeffect{Output ``letters'' by analogical mapping of compositional structure}
\end{examplebox}

\mechfooter{\statusobs}{abstract-concepts, pattern-completion, semantic-integration}

%-----------------------------------------------------------------------------
\subsubsection{Causal Inference Mechanism}
\label{mech:causal-inference}

\noindent\depthinfo{0.40--0.75 (M-L)} | \primaryimpl{MLP neurons + Attention heads} | \litnames{cause-effect, causal reasoning, counterfactual reasoning, mathematical reasoning}

\begin{functiondesc}
Understand and reason about causal relationships. Distinguish causation from correlation. Predict effects from causes and infer causes from effects. Support counterfactual reasoning: what would happen if conditions changed. Enable temporal and mechanistic causal understanding. Integrate causal knowledge from training. Support explanation generation through causal chains. Include quantitative causal relationships through mathematical computation: proportions, rates, numerical relationships within causal frameworks.
\end{functiondesc}

\begin{implementationbox}
\mlpimpl{Causal-knowledge neurons (M-L, 0.50--0.75) encode causal relationships and mechanisms learned from training data. Arithmetic neurons (M-L, 0.40--0.70) implement mathematical operations for quantitative causal relationships.}\\
\attnimpl{Causal-linking heads (M-L, 0.55--0.70) attend to causally related events and connect causes to effects. Mathematical-context heads (M, 0.45--0.65) recognize mathematical structures and numerical patterns.}\\
\circuitimpl{Event identification $\rightarrow$ causal relationship retrieval $\rightarrow$ inference (with quantitative computation) $\rightarrow$ prediction or explanation. Mathematical computation integrated as quantitative component.}
\end{implementationbox}

\begin{ablationbox}
\textbf{Expected ablation:} Moderate loss of causal understanding. Increased confusion between correlation and causation. Notable degradation on counterfactual reasoning. Reduced ability to explain mechanisms. More associative than causal thinking. Loss of quantitative reasoning within causal contexts. Difficulty with proportional relationships.
\end{ablationbox}

\begin{examplebox}
\exinput{``Why did the plant die?'' [Context: no water for weeks]}\\
\exbehavior{Retrieve causal knowledge: lack of water causes plant death, apply mechanism understanding}\\
\exeffect{Output explanation through causal reasoning, not just correlation}

\vspace{0.3em}
\exinput{``If 3 apples cost \$2.40, how much do 5 apples cost?''}\\
\exbehavior{Identify proportional causal relationship, compute (2.40/3)*5 using arithmetic neurons}\\
\exeffect{Output ``\$4.00'' through quantitative causal reasoning}
\end{examplebox}

\mechfooter{\statusobs}{factual-recall, multi-step-reasoning, semantic-integration}

%=============================================================================
\subsection{Information Routing Stack}
\label{sec:routing-stack}

\textbf{Stack overview:} Determine which information is relevant and route attention accordingly. Filter content, focus on salient elements, manage task-appropriate processing. Enable selective information processing and dynamic strategy selection.

%-----------------------------------------------------------------------------
\subsubsection{Relevance Filtering}
\label{mech:relevance-filtering}

\noindent\depthinfo{0.35--0.60} | \primaryimpl{Attention heads} | \litnames{relevance computation, salience detection, information filtering}

\begin{functiondesc}
Identify relevant information from context and filter irrelevant content. Compute relevance scores based on semantic similarity, task alignment, and topical coherence. Maintain topic coherence by attending to topic-establishing phrases and domain indicators. Enable focused processing in long contexts with diverse content. Support downstream mechanisms by pre-filtering information flow.
\end{functiondesc}

\begin{implementationbox}
\attnimpl{Topic-relevance heads (M, 0.35--0.60) compute semantic similarity between query/topic and context elements. Attend strongly to task-relevant content while downweighting unrelated information.}\\
\circuitimpl{Early filtering (M) $\rightarrow$ focused attention (L) $\rightarrow$ output generation. Hierarchical refinement of attention allocation.}
\end{implementationbox}

\begin{ablationbox}
\textbf{Expected ablation:} Moderate reduction in focus with increased topic drift. Model distracted by irrelevant content. Notable degradation on long contexts with mixed topics. Responses wander off-topic or incorporate peripheral details inappropriately.
\end{ablationbox}

\begin{examplebox}
\exinput{``[Document about cars, climate, history] What caused the 2008 financial crisis?''}\\
\exbehavior{Identify financial/economic content as relevant, de-emphasize unrelated topics}\\
\exeffect{Focus processing on economic information, ignore cars and climate content}
\end{examplebox}

\mechfooter{\statuswell}{focused-attention, task-routing}

%-----------------------------------------------------------------------------
\subsubsection{Focused Attention}
\label{mech:focused-attention}

\noindent\depthinfo{0.65--0.80} | \primaryimpl{Attention heads} | \litnames{attention focusing, selective attention, spotlight mechanism}

\begin{functiondesc}
Concentrate attention on most salient elements for immediate generation step. Implement dynamic focus allocation: suppress less important content, amplify critical information. More selective than relevance filtering. Determine exactly which tokens should influence next token prediction. Shift focus dynamically as generation proceeds. Enable precise, targeted responses rather than diffuse answers.
\end{functiondesc}

\begin{implementationbox}
\attnimpl{Focus heads (L, 0.65--0.80) implement highly selective attention patterns, attending to 5--20\% of context. Dynamically adjust focus based on generation state and query emphasis.}\\
\circuitimpl{Refinement of earlier relevance filtering. Works in late layers after content understanding established.}
\end{implementationbox}

\begin{ablationbox}
\textbf{Expected ablation:} Moderate reduction in focus precision. Model gives more equal weight to important and peripheral information. Notable degradation on targeted responses requiring precise answers. Answers become more diffuse, less direct, include unnecessary details.
\end{ablationbox}

\begin{examplebox}
\exinput{``Among all the details provided, what is the MAIN cause?''}\\
\exbehavior{Attend to ``MAIN cause'' emphasis, suppress secondary factors and background}\\
\exeffect{Produce direct answer focusing on primary cause, not comprehensive list}
\end{examplebox}

\mechfooter{\statuswell}{relevance-filtering, task-routing}

%-----------------------------------------------------------------------------
\subsubsection{Task Routing}
\label{mech:task-routing}

\noindent\depthinfo{0.70--0.85} | \primaryimpl{Attention heads} | \litnames{task classification, strategy selection, query dispatching}

\begin{functiondesc}
Route different query types to appropriate processing strategies and knowledge domains. Act as dispatchers recognizing query type: factual vs. creative vs. analytical vs. procedural. Bias downstream processing toward suitable approaches. Activate different computation paths based on task classification. Enable dynamic strategy selection without explicit instruction. Support task-appropriate response generation.
\end{functiondesc}

\begin{implementationbox}
\attnimpl{Router heads (L, 0.70--0.85) detect task-type indicators and query structure. Influence downstream layer processing through attention-mediated routing signals.}\\
\circuitimpl{Task classification (L) $\rightarrow$ strategy-specific processing (L-F) $\rightarrow$ appropriate output generation. Soft routing via attention rather than hard gating.}
\end{implementationbox}

\begin{ablationbox}
\textbf{Expected ablation:} Moderate reduction in task-appropriate processing. Suboptimal strategy selection. Creative approaches for factual queries or vice versa. Notable degradation on diverse query types requiring different processing modes. Reduced adaptability to query characteristics.
\end{ablationbox}

\begin{examplebox}
\exinput{``What is the capital of France?'' vs. ``Write a poem about Paris''}\\
\exbehavior{Route first to factual retrieval pathways, second to creative generation}\\
\exeffect{Factual answer vs. creative content with appropriate processing}
\end{examplebox}

\mechfooter{\statuswell}{focused-attention, output-formatting}

%-----------------------------------------------------------------------------
\subsubsection{Long-Range Dependency Tracking}
\label{mech:long-range-dependency}

\noindent\depthinfo{0.40--0.65} | \primaryimpl{Attention heads} | \litnames{long-distance attention, dependency maintenance, distant connection}

\begin{functiondesc}
Maintain connections between syntactically or semantically related elements across large distances (20--100+ tokens). Track dependencies without degradation over distance. Implement transformer advantage over RNNs: direct long-distance connections. Support nested structures, long-distance agreement, and complex syntactic relationships. Maintain multiple simultaneous long-range connections.
\end{functiondesc}

\begin{implementationbox}
\attnimpl{Long-range dependency heads (M, 0.40--0.65) attend across large token distances to maintain syntactic and semantic relationships. Relatively flat attention distribution over distant elements.}\\
\circuitimpl{Parallel to local processing. Maintains global structural information while other mechanisms process local patterns.}
\end{implementationbox}

\begin{ablationbox}
\textbf{Expected ablation:} Notable degradation on complex sentences and long-range relationships. Significant performance loss on long-distance syntactic agreement. Severe impact on nested structures and embedded clauses. Model treats distant elements as independent.
\end{ablationbox}

\begin{examplebox}
\exinput{``The book [that Alice mentioned [that Bob recommended]] was excellent.''}\\
\exbehavior{``was'' attends to ``book'' across two levels of embedding}\\
\exeffect{Maintain subject-verb agreement despite intervening clauses}
\end{examplebox}

\mechfooter{\statusobs}{coreference-resolution, entity-tracking}

%-----------------------------------------------------------------------------
\subsubsection{Context Aggregation}
\label{mech:context-aggregation}

\noindent\depthinfo{0.50--0.75} | \primaryimpl{Attention heads + MLP neurons} | \litnames{global context, background integration, discourse modeling}

\begin{functiondesc}
Aggregate broad contextual information to inform generation. Build global representation of discourse state, topic, and background. Compute context vectors summarizing overall input characteristics. Complement focused attention with background awareness. Enable context-appropriate generation without explicitly attending to all details. Support discourse coherence and stylistic consistency.
\end{functiondesc}

\begin{implementationbox}
\attnimpl{Context aggregation heads (M-L, 0.50--0.70) implement broad, diffuse attention patterns across large context windows. Relatively uniform attention to build aggregate representations.}\\
\mlpimpl{Context integration neurons (L, 0.60--0.75) process aggregated context to extract high-level features: topic, style, formality level, domain.}
\end{implementationbox}

\begin{ablationbox}
\textbf{Expected ablation:} Moderate loss of global coherence and context-awareness. Responses technically correct but contextually inappropriate. Notable degradation in style consistency and discourse-level coherence. Reduced sensitivity to overall document characteristics.
\end{ablationbox}

\begin{examplebox}
\exinput{``[Long technical document in formal style]... In summary,''}\\
\exbehavior{Aggregate stylistic and domain information from entire context}\\
\exeffect{Generate summary matching technical formality and domain vocabulary}
\end{examplebox}

\mechfooter{\statusobs}{focused-attention, output-formatting}

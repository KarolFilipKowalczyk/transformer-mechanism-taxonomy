\documentclass[11pt,a4paper]{article}

% Load preamble with packages and macros
\input{preamble}

% Document metadata
\title{Transformer Mechanisms: A Functional Taxonomy}
\author{Karol Kowalczyk}
\date{November 2025}

\begin{document}

\maketitle

\begin{abstract}
Transformer models implement computation through mechanisms that span attention heads, MLP layers, and multi-component circuits. Current mechanistic interpretability research suffers from fragmented naming: the same mechanism appears under different names across research traditions, while identical names refer to distinct computational functions. This work proposes a mechanism-first taxonomy organizing 35 computational mechanisms into 8 functional stacks, with each mechanism described independently of its implementation substrate. The taxonomy enables cross-architecture comparison, supports multi-component integration, and provides stable vocabulary synthesizing findings from attention analysis, MLP studies, circuit tracing, and sparse autoencoder research.
\end{abstract}

\tableofcontents
\clearpage

%=============================================================================
% INTRODUCTION
%=============================================================================

\section{Introduction}
\label{sec:introduction}

\subsection{The Problem: Fragmented Mechanism Naming}

Mechanistic interpretability has identified induction heads enabling few-shot learning~\cite{olsson2022context}, characterized MLPs as key-value memories~\cite{geva2021transformer}, traced multi-component circuits like the IOI circuit~\cite{wang2022interpretability}, and extracted monosemantic features via sparse autoencoders~\cite{bricken2023towards}. Yet naming conventions remain inconsistent and component-centric.

The same computational mechanism appears under multiple names: pattern completion is called "induction head," "pattern head," "copy head," or "ICL head" depending on research tradition. Conversely, "copy head" refers to at least three distinct mechanisms: pattern completion, name-mover routing, and position-based copying.

The root problem is \textbf{component-first thinking}: organizing by implementation substrate (attention heads, MLP neurons) rather than computational function. This creates artificial boundaries preventing systematic comparison across models and obscuring relationships between components and the mechanisms they implement.

\subsection{The Solution: Mechanism-First Organization}

This taxonomy applies three organizing principles:

\textbf{1. Separate mechanism from implementation.} Each mechanism describes computational transformation independently, then maps to implementing components (attention heads, MLPs, circuits, SAE features).

\textbf{2. Organize by computational function.} Eight functional stacks reflect the transformer pipeline: pattern detection, memory retrieval, routing, transformation, reasoning, safety, output control, and composition.

\textbf{3. Support multi-component integration.} Sophisticated mechanisms require coordinated computation across attention, MLPs, and depth. The taxonomy explicitly represents this composition.

\subsection{Scope and Contributions}

This work catalogs 35 mechanisms meeting four criteria: (1) observed across multiple models, (2) validated through ablation, (3) mechanistically understood at component level, (4) reproducible using standard techniques.

\textbf{Key contributions:}
\begin{itemize}[itemsep=0.2em]
\item Standardized vocabulary bridging attention, MLP, circuit, and SAE research traditions
\item Functional organization revealing transformer computational architecture
\item Multi-component integration patterns for sophisticated mechanisms
\item Implementation specifications enabling cross-architecture comparison
\item Empirically grounded descriptions synthesizing interpretability findings
\end{itemize}

\subsection{Document Structure}

Section~\ref{sec:framework} explains the mechanism-first framework and depth-based organization. Sections~\ref{sec:pattern-stack} through~\ref{sec:composition-stack} catalog mechanisms by functional stack. Section~\ref{sec:discussion} examines patterns, limitations, and future directions.

%=============================================================================
% FRAMEWORK
%=============================================================================

\section{Conceptual Framework}
\label{sec:framework}

\subsection{Mechanism vs. Component}

\textbf{Mechanisms} describe computational transformations independently of substrate: pattern completion, factual recall, output routing. \textbf{Components} are architectural primitives: attention heads, MLP layers, circuits. The relationship is many-to-many: mechanisms typically require multiple components, and components contribute to multiple mechanisms.

\textbf{Three organizational levels:}
\begin{enumerate}[itemsep=0.2em]
\item \textbf{Attention heads} implement learned routing patterns (previous-token, induction, name-mover)
\item \textbf{MLP layers} implement associative memory and nonlinear transformation
\item \textbf{Circuits} integrate attention and MLPs across layers into complex behaviors
\end{enumerate}

\subsection{Depth-Based Organization}

Transformer layers exhibit systematic functional specialization across depth. This taxonomy uses relative depth notation: $d_{\text{rel}} = l/L$ where $l$ is layer index and $L$ is total layers.

\textbf{Four depth regions:}
\begin{itemize}[itemsep=0.2em]
\item \textbf{Early (E, 0.00--0.25):} Surface processing, syntactic patterns, instruction markers
\item \textbf{Middle (M, 0.25--0.70):} Core computation, facts, entities, reasoning
\item \textbf{Late (L, 0.70--0.88):} Integration, output routing, strategy selection
\item \textbf{Final (F, 0.88--1.00):} Constraint enforcement, safety, format control
\end{itemize}

This enables architecture-independent mechanism descriptions: a mechanism at 0.40 relative depth occupies similar functional space in 12-layer (layer 5), 48-layer (layer 19), and 96-layer (layer 38) models.

\subsection{Methodology and Evidence Standards}

Mechanisms are identified through converging evidence from five techniques: attention pattern analysis, ablation studies, activation patching, logit attribution, and sparse autoencoder analysis.

All mechanisms in this taxonomy are validated through systematic empirical investigation. However, the depth of mechanistic understanding varies significantly. To address this, we employ a tier system reflecting evidence quality and a formal independence framework.

\subsection{Mechanism Independence Criteria}

A computational pattern qualifies as an independent mechanism if it meets \textbf{all four criteria}:

\textbf{1. Distinct Ablation Signature:} Removal produces unique behavioral deficits not fully explained by ablating other mechanisms. The mechanism must have identifiable functional consequences that distinguish it from related mechanisms.

\textit{Example:} Removing induction heads specifically impairs few-shot learning while preserving other pattern-matching capabilities. This distinct signature establishes independence from general pattern detection.

\textbf{2. Separable Implementation:} Can be localized to specific components or circuits that are not completely shared with other mechanisms. The mechanism may share some components but must have unique implementing structure.

\textit{Example:} The IOI circuit shares duplicate-token heads with pattern completion but has unique S-inhibition and name-mover coordination, establishing separable implementation.

\textbf{3. Independent Activation:} Can be activated or suppressed independently of related mechanisms through targeted interventions. The mechanism must be controllable without necessarily affecting all related computations.

\textit{Example:} Induction heads can be ablated without removing all duplicate-token functionality, demonstrating independent activation control.

\textbf{4. Non-Compositional:} Not fully explainable as a simple composition of other documented mechanisms. Emergent behaviors from mechanism interaction don't qualify as separate mechanisms unless they develop distinct implementation.

\textit{Counterexample:} Memory consolidation appears to be the emergent composition of factual recall + schema retrieval + entity grounding, lacking unique implementing structure. It fails the non-compositional criterion.

These criteria, informed by causal abstraction frameworks~\cite{geiger2021causal} and rigorous testing methodologies~\cite{chan2022causal}, provide systematic standards for mechanism identification and prevent taxonomic inflation from classifying every behavioral pattern as an independent mechanism.

\subsection{Confidence Tiers}

Mechanisms are categorized by the strength and type of empirical support:

\textbf{Tier 1 - Mechanistic Understanding:}
\begin{itemize}[itemsep=0.1em]
\item Circuit-level understanding with identified implementing components
\item Extensive ablation studies demonstrating causal role across multiple models
\item Reproducible using standard interpretability techniques
\item Clear implementation specifications enabling cross-architecture identification
\item \textit{Examples:} Induction mechanism, IOI circuit, factual recall, position-based processing
\end{itemize}

\textbf{Tier 2 - Behavioral Evidence:}
\begin{itemize}[itemsep=0.1em]
\item Consistent behavioral evidence across models and contexts
\item Partial mechanistic understanding with some component identification
\item Preliminary ablation or intervention studies showing functional role
\item Implementation details partially characterized but incomplete
\item \textit{Examples:} Algorithmic continuation, analogical mapping, causal inference, most reasoning mechanisms
\end{itemize}

\textbf{Tier 3 - Speculative (Appendix Only):}
\begin{itemize}[itemsep=0.1em]
\item Reasonable hypotheses with preliminary behavioral observations
\item Insufficient mechanistic characterization for main taxonomy inclusion
\item May be emergent behaviors from other mechanism composition
\item Require substantial additional research for validation
\item Relocated to Appendix A: "Emerging Mechanisms Under Investigation"
\end{itemize}

\textbf{Current taxonomy composition:} 33 mechanisms in main taxonomy (11 Tier 1, 22 Tier 2), with 3 mechanisms relocated to appendix. This distribution reflects the current state of mechanistic interpretability: strong understanding of core attention-based mechanisms, behavioral characterization of many MLP and reasoning mechanisms, and substantial work remaining to achieve complete mechanistic understanding across all computational functions.

%=============================================================================
% MECHANISM STACKS OVERVIEW
%=============================================================================

\section{Eight Functional Stacks}
\label{sec:stacks-overview}

The taxonomy organizes 33 mechanisms into eight functional stacks:

\textbf{1. Pattern \& Sequential Stack} (3 mechanisms, 0.05--0.65): Pattern detection and completion, sequence continuation, positional processing.

\textbf{2. Memory \& Knowledge Stack} (5 mechanisms, 0.35--0.85): Factual recall, entity grounding, schema retrieval, output routing.

\textbf{3. Routing \& Context Stack} (4 mechanisms, 0.35--0.75): Relevance filtering, attention focusing, task routing, context aggregation.

\textbf{4. Feature Transformation Stack} (3 mechanisms, 0.20--0.80): Nonlinear composition, abstract concept formation, semantic integration.

\textbf{5. Reasoning \& Inference Stack} (5 mechanisms, 0.40--0.88): Multi-step reasoning, planning, consistency checking, analogical mapping, causal inference.

\textbf{6. Safety \& Policy Stack} (5 mechanisms, 0.05--0.98): Harmful content detection, policy enforcement, refusal generation, jailbreak resistance.

\textbf{7. Output \& Quality Stack} (4 mechanisms, 0.35--0.98): Format enforcement, style modulation, explanation generation, completion control.

\textbf{8. Composition \& Integration Stack} (4 mechanisms, system-level): Cross-layer circuits, multi-head coordination, attention-MLP composition, representational superposition.

%=============================================================================
% MECHANISM CATALOG
%=============================================================================

\section{Mechanism Catalog}
\label{sec:catalog}

This section catalogs mechanisms using a compressed format: \textbf{Function} describes computational transformation, \textbf{Implementation} specifies components, \textbf{Ablation} summarizes behavioral changes when removed, \textbf{Example} provides concrete inputâ†’mechanismâ†’output.

% Individual stack files
\input{stack_pattern_sequential}
\input{stack_memory_knowledge}
\input{stack_routing_context}
\input{stack_feature_transformation}
\input{stack_reasoning_inference}
\input{stack_safety_policy}
\input{stack_output_quality}
\input{stack_composition_integration}

%=============================================================================
% DISCUSSION
%=============================================================================

\section{Discussion}
\label{sec:discussion}

\subsection{Key Patterns}

\textbf{Component specialization:} Attention specializes in routing (15 attention-primary mechanisms), MLPs in storage and transformation (21 MLP-required mechanisms). Sophisticated behaviors require multi-component circuits spanning 5--30 layers.

\textbf{Polyfunctionality:} Components contribute to multiple mechanisms depending on context. Duplicate-token heads serve pattern completion, entity tracking, and output routing. This computational reuse enables efficiency but complicates attribution.

\textbf{Depth-based cascades:} Mechanisms compose across depth in systematic patterns:
\begin{itemize}[itemsep=0.1em]
\item Earlyâ†’Middle: Surface features prepare semantic processing
\item Middleâ†’Late: Retrieved information feeds integration and routing
\item Lateâ†’Final: Content generation meets constraint enforcement
\end{itemize}

\subsection{Relationship to Prior Work}

This taxonomy integrates findings from:
\begin{itemize}[itemsep=0.1em]
\item Attention head taxonomies~\cite{zheng2025attention}: incorporated as mechanism implementations
\item MLP memory frameworks~\cite{geva2021transformer,geva2023dissecting}: integrated into full retrieval circuits
\item Circuit analyses~\cite{wang2022interpretability}: generalized into mechanism compositions
\item SAE feature catalogs~\cite{bricken2023towards}: connected to mechanism components
\end{itemize}

The contribution is integration through mechanism-first organization.

\subsection{The MLP Characterization Gap}

This taxonomy reveals significant asymmetry in mechanistic understanding between attention and MLP components:

\textbf{Attention mechanisms} (15 total, including attention-primary mechanisms): Well-characterized through attention pattern analysis, head-specific ablation, and circuit tracing. Tier 1 mechanisms like induction and IOI circuits have detailed component-level understanding with traced information flow across layers.

\textbf{MLP mechanisms} (18 mechanisms involve MLPs, 12 MLP-primary): Characterized primarily through:
\begin{itemize}[itemsep=0.1em]
\item Behavioral observation and ablation at layer level
\item Theoretical understanding of MLP function (key-value memory framework~\cite{geva2021transformer})
\item Sparse autoencoder feature analysis~\cite{bricken2023towards}
\item Neuron activation studies and knowledge localization~\cite{meng2022locating}
\end{itemize}

However, MLP mechanisms lack the circuit-level tracing available for attention. We understand \textit{what} MLPs compute (factual recall, semantic integration, abstract concepts) primarily through behavioral inference and theoretical frameworks rather than traced information flow through specific neuron populations.

This gap explains why many MLP-involving mechanisms are Tier 2 (behavioral evidence) rather than Tier 1 (mechanistic understanding). Factual recall achieves Tier 1 status through extensive neuron-level studies, but most other MLP mechanisms remain at Tier 2. Future research should prioritize MLP circuit tracing to achieve parity with attention mechanism understanding.

\textbf{Promising directions:}
\begin{itemize}[itemsep=0.1em]
\item Gradient-based MLP circuit attribution techniques
\item Feature-level ablation using sparse autoencoder features as units
\item Cross-layer MLP composition analysis tracking feature flow
\item Systematic neuron population studies identifying functional clusters
\item Causal tracing of MLP contributions to specific behavioral outcomes
\end{itemize}

\subsection{Limitations}

\textbf{Incomplete coverage:} Many capabilities (strategic planning, creative generation, abstract reasoning) lack mechanistic understanding.

\textbf{Tier imbalance:} Only 33\% of mechanisms achieve Tier 1 (mechanistic understanding). The remaining 67\% rely primarily on behavioral evidence. This reflects the current state of interpretability research but indicates substantial work remains to achieve full mechanistic characterization across all computational functions.

\textbf{Independence criteria challenges:} Some included mechanisms may fail strict independence tests under future scrutiny. The four-criterion framework provides structure but borderline cases remain. For example, is Focused Attention truly independent of Relevance Filtering, or a refinement stage of the same mechanism? Further research may reveal that some Tier 2 mechanisms are compositional rather than independent.

\textbf{Empirical gaps:} Several Tier 2 mechanisms need stronger validation through systematic ablation studies and component identification. The behavioral evidence is consistent but mechanistic understanding incomplete.

\textbf{Architecture dependence:} Focus on decoder-only transformers; generalization to other architectures requires investigation.

\textbf{Dynamic activation:} Mechanisms activate conditionally based on context; static descriptions don't capture this context-dependent engagement.

\textbf{Mechanism interactions:} Interference patterns between mechanisms (safety vs. helpfulness, format vs. reasoning) remain undercharacterized.

\subsection{Future Directions}

\textbf{Automated detection:} Develop tools to identify mechanisms in new models using taxonomy as specification.

\textbf{Complete MLP characterization:} Systematic study of MLP functions beyond key-value memory.

\textbf{Circuit composition rules:} Formalize patterns governing how mechanisms compose into sophisticated behaviors.

\textbf{SAE-mechanism integration:} Map sparse autoencoder features to mechanism components systematically.

\textbf{Cross-architecture transfer:} Test mechanism generalization to encoders, encoder-decoders, and alternative architectures.

\textbf{Mechanism editing:} Use understanding to improve capabilities through targeted interventions.

%=============================================================================
% CONCLUSION
%=============================================================================

\section{Conclusion}
\label{sec:conclusion}

This taxonomy provides mechanistic interpretability with standardized vocabulary bridging research traditions, functional organization revealing computational architecture, explicit multi-component integration patterns, and formal independence criteria establishing rigorous standards for mechanism identification. The framework enables researchers to communicate about computational functions while maintaining precision about implementations and evidence quality.

The taxonomy catalogs 33 mechanisms (11 Tier 1, 22 Tier 2) meeting independence criteria, with 3 additional patterns relocated to an appendix for future investigation. This distribution honestly reflects the current state of the field: strong mechanistic understanding of core attention-based mechanisms, behavioral characterization of MLP and reasoning mechanisms, and substantial work remaining to achieve complete circuit-level understanding.

\textbf{Adoption guidelines:}
\begin{enumerate}[itemsep=0.2em]
\item Use canonical mechanism names with literature cross-references
\item Specify implementation details: components, depths, circuit patterns
\item Indicate evidence tier (Tier 1: Mechanistic, Tier 2: Behavioral)
\item Apply independence criteria when proposing new mechanisms
\item Map components to mechanisms they implement
\item Use relative depth notation for cross-architecture comparison
\end{enumerate}

The taxonomy represents progress toward complete mechanistic understanding: explaining transformer capabilities through mechanism composition from architectural primitives. As interpretability research continues, mechanism-first organization with rigorous independence criteria will integrate findings into cumulative mechanistic knowledge while maintaining appropriate epistemic humility about evidence limitations.

%=============================================================================
% APPENDIX
%=============================================================================
\input{appendix_emerging}

%=============================================================================
% BIBLIOGRAPHY
%=============================================================================
\clearpage
\bibliographystyle{plainnat}
\bibliography{bibliography}

\end{document}

%=============================================================================
\subsection{Infrastructure Stack}
\label{sec:infrastructure-stack}

\textbf{Stack overview:} Provide foundational architectural mechanisms enabling transformer operation. Support gradient flow, numerical stability, information propagation, and training dynamics. Essential infrastructure without direct computational semantics.

%-----------------------------------------------------------------------------
\subsubsection{Residual Connections}
\label{mech:residual-connections}

\noindent\depthinfo{All layers} | \primaryimpl{Architecture} | \litnames{skip connections, residual stream, additive composition}

\begin{functiondesc}
Enable information flow through depth via additive skip connections. Each layer adds contribution to residual stream rather than overwriting: $h_{l+1} = h_l + f(h_l)$. Provide gradient highways enabling training of very deep networks. Allow early layer information to reach late layers directly. Enable layers to learn incremental refinements rather than complete transformations. Support composition of mechanisms across depth. Prevent vanishing gradients through direct paths. Foundation for all transformer computation and learning.
\end{functiondesc}

\begin{implementationbox}
\archimpl{Applied after every attention and MLP block: $h = h + \text{Attn}(h)$; $h = h + \text{MLP}(h)$. Direct path from input to any layer. Gradients flow backwards through skip connections during training.}\\
\circuitimpl{Universal architectural pattern. Enables all cross-layer circuits and mechanism composition. Without residual connections, transformers cannot train or function effectively.}
\end{implementationbox}

\begin{ablationbox}
\textbf{Expected ablation:} Catastrophic failure. Training impossible for deep networks without gradient highways. Information cannot flow through depth. Layers would need to learn full transformations rather than refinements. Fundamental architectural requirement.
\end{ablationbox}

\begin{examplebox}
\exinput{[Deep network with 96 layers]}\\
\exbehavior{Information from layer 1 can reach layer 96 through residual paths}\\
\exeffect{Trainable deep networks with gradient flow}
\end{examplebox}

\mechfooter{\statuswell}{layer-normalization, attention-mlp-composition}

%-----------------------------------------------------------------------------
\subsubsection{Layer Normalization}
\label{mech:layer-normalization}

\noindent\depthinfo{All layers} | \primaryimpl{Architecture} | \litnames{LayerNorm, activation normalization, scale stabilization}

\begin{functiondesc}
Normalize activation distributions to stabilize training and inference. Apply normalization before each attention and MLP block. Rescale and recenter activations to zero mean and unit variance: $\text{LN}(x) = \gamma \frac{x - \mu}{\sigma} + \beta$. Prevent activation explosion or vanishing through depth. Enable stable gradient flow. Provide consistent activation scales for all mechanisms. Support training of very deep networks. Learned scale ($\gamma$) and shift ($\beta$) parameters adapt normalization to task.
\end{functiondesc}

\begin{implementationbox}
\archimpl{Applied before attention and MLP: $\text{Attn}(\text{LN}(h))$ and $\text{MLP}(\text{LN}(h))$. Normalizes across feature dimension independently for each position and sequence.}\\
\circuitimpl{Preprocessing for all computational blocks. Essential for numerical stability. Affects learned representations through normalization effects.}
\end{implementationbox}

\begin{ablationbox}
\textbf{Expected ablation:} Severe training instability. Activation explosion or vanishing in deep networks. Training divergence or failure to learn. In trained models: extreme sensitivity to inputs, unpredictable behavior. Critical for both training and inference stability.
\end{ablationbox}

\begin{examplebox}
\exinput{[Activations with varying magnitudes: $10^{-3}$ to $10^3$]}\\
\exbehavior{Normalize to zero mean and unit variance before processing}\\
\exeffect{Stable computation regardless of input activation scales}
\end{examplebox}

\mechfooter{\statuswell}{residual-connections, gradient-flow}

%-----------------------------------------------------------------------------
\subsubsection{Attention Masking}
\label{mech:attention-masking}

\noindent\depthinfo{All layers} | \primaryimpl{Architecture} | \litnames{causal masking, autoregressive mask, future blocking}

\begin{functiondesc}
Prevent attention to future tokens, enforcing causal/autoregressive generation. Implement masking: position $i$ can only attend to positions $\leq i$. Enable left-to-right generation without information leakage from future. Support autoregressive training and inference. Distinguish training (teacher forcing with masking) from inference (sequential generation). Foundation for language modeling objective. Enable models to predict next token without seeing future context.
\end{functiondesc}

\begin{implementationbox}
\archimpl{Attention mask applied before softmax: $\text{softmax}(\frac{QK^T}{\sqrt{d_k}} + M)$ where $M_{ij} = -\infty$ if $j > i$. Implemented efficiently as triangular mask.}\\
\circuitimpl{Universal constraint on all attention operations in autoregressive models. Bidirectional models (BERT-style) omit this masking.}
\end{implementationbox}

\begin{ablationbox}
\textbf{Expected ablation:} Complete failure of autoregressive generation. Model would see future tokens during training, learning invalid patterns. During inference, model cannot generate coherently without causal structure. Fundamental requirement for language generation models.
\end{ablationbox}

\begin{examplebox}
\exinput{Training: ``The cat sat on the mat'' [predict each token]}\\
\exbehavior{When predicting ``sat'', mask prevents seeing ``on'', ``the'', ``mat''}\\
\exeffect{Learn valid next-token prediction without future information leakage}
\end{examplebox}

\mechfooter{\statuswell}{positional-encoding, attention-computation}

%-----------------------------------------------------------------------------
\subsubsection{Embedding and Unembedding}
\label{mech:embedding-unembedding}

\noindent\depthinfo{Input and output} | \primaryimpl{Architecture} | \litnames{token embedding, vocabulary projection, logit computation}

\begin{functiondesc}
Convert between discrete tokens and continuous representations. Embedding: map token IDs to dense vectors in model dimension. Provide learned token representations as input to transformer. Unembedding: project final hidden state to vocabulary logits. Compute probability distribution over next tokens. Often tie embedding and unembedding matrices for parameter efficiency. Enable interface between discrete token space and continuous activation space.
\end{functiondesc}

\begin{implementationbox}
\archimpl{Embedding: $h_0 = E[x]$ where $E \in \mathbb{R}^{V \times d}$ is learned embedding matrix. Unembedding: $\text{logits} = h_L W_U$ where $W_U \in \mathbb{R}^{d \times V}$ (often $W_U = E^T$). Vocabulary size $V$ typically 32K--100K.}\\
\circuitimpl{Input/output interface for all transformer computation. All mechanisms operate on embedded representations.}
\end{implementationbox}

\begin{ablationbox}
\textbf{Expected ablation:} Complete model failure. Cannot process discrete tokens without embedding. Cannot generate predictions without unembedding. Fundamental input/output interface. Poor embeddings degrade all downstream computation.
\end{ablationbox}

\begin{examplebox}
\exinput{Token sequence: [``The'', ``cat'', ``sat'']}\\
\exbehavior{Embed to dense vectors $\rightarrow$ process through layers $\rightarrow$ unembed to vocabulary logits}\\
\exeffect{Interface between discrete language and continuous computation}
\end{examplebox}

\mechfooter{\statuswell}{positional-encoding, residual-connections}

%-----------------------------------------------------------------------------
\subsubsection{Attention Computation}
\label{mech:attention-computation}

\noindent\depthinfo{All layers} | \primaryimpl{Architecture} | \litnames{scaled dot-product attention, softmax attention, QKV attention}

\begin{functiondesc}
Implement core attention operation: weighted aggregation based on learned queries, keys, and values. Compute similarity between queries and keys: $\text{score}_{ij} = \frac{q_i \cdot k_j}{\sqrt{d_k}}$. Apply softmax for normalized weights: $\alpha_{ij} = \text{softmax}(\text{score}_i)_j$. Aggregate values: $\text{output}_i = \sum_j \alpha_{ij} v_j$. Enable learned routing patterns. Support content-based and position-based attention. Foundation for all attention-based mechanisms. Scaled by $\sqrt{d_k}$ for numerical stability.
\end{functiondesc}

\begin{implementationbox}
\archimpl{$\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$ where $Q = h W_Q$, $K = h W_K$, $V = h W_V$ are learned projections. Efficient matrix implementation for parallelization.}\\
\circuitimpl{Fundamental operation underlying all attention heads and mechanisms. Repeated in every attention head of every layer.}
\end{implementationbox}

\begin{ablationbox}
\textbf{Expected ablation:} Cannot ablate (foundational operation). Removing attention computation removes all attention mechanisms. Model reduced to MLP-only architecture with fundamentally different capabilities. Attention is core transformer innovation.
\end{ablationbox}

\begin{examplebox}
\exinput{[Hidden states for sequence]}\\
\exbehavior{Compute query-key similarities, normalize, aggregate values based on attention weights}\\
\exeffect{Content-based weighted aggregation enabling selective information routing}
\end{examplebox}

\mechfooter{\statuswell}{multi-head-composition, attention-masking}

%-----------------------------------------------------------------------------
\subsubsection{Gradient Flow}
\label{mech:gradient-flow}

\noindent\depthinfo{Training infrastructure} | \primaryimpl{Architecture + Training} | \litnames{backpropagation, gradient propagation, learning dynamics}

\begin{functiondesc}
Enable error gradient propagation from output to input during training. Implement backpropagation through all layers and operations. Support efficient gradient computation through automatic differentiation. Maintain gradient magnitude through depth via residual connections and normalization. Prevent vanishing or exploding gradients. Enable learning of early and late layer parameters. Support optimization algorithms (Adam, SGD) to update parameters. Foundation for all model learning and capability acquisition.
\end{functiondesc}

\begin{implementationbox}
\archimpl{Backpropagation through computational graph. Residual connections provide gradient highways. LayerNorm prevents gradient explosion/vanishing. Gradient clipping prevents instability. Optimization algorithm (typically AdamW) updates parameters: $\theta_{t+1} = \theta_t - \eta \nabla_\theta L$.}\\
\circuitimpl{Training-time mechanism enabling parameter learning. Not active during inference but shapes all learned mechanisms.}
\end{implementationbox}

\begin{ablationbox}
\textbf{Expected ablation:} Cannot train model without gradient flow. No learning possible. Parameters remain random. Zero capability acquisition. Fundamental requirement for creating useful models through training.
\end{ablationbox}

\begin{examplebox}
\exinput{[Training: compute loss from predictions]}\\
\exbehavior{Backpropagate gradients through all layers, update parameters to reduce loss}\\
\exeffect{Model learns capabilities through gradient-based optimization}
\end{examplebox}

\mechfooter{\statuswell}{residual-connections, layer-normalization}

%-----------------------------------------------------------------------------
\subsubsection{Tokenization}
\label{mech:tokenization}

\noindent\depthinfo{Preprocessing} | \primaryimpl{External to model} | \litnames{BPE, subword tokenization, vocabulary}

\begin{functiondesc}
Convert text into discrete token sequences suitable for model processing. Implement subword tokenization (BPE, WordPiece): balance vocabulary size with token granularity. Handle rare words through subword decomposition. Enable consistent length sequences through padding/truncation. Map between text space and token space. Learned vocabulary (32K--100K tokens) from training corpus. Preprocessing step before embedding. Affects model capabilities through vocabulary coverage and granularity.
\end{functiondesc}

\begin{implementationbox}
\archimpl{External to neural model. Typically BPE (Byte Pair Encoding) algorithm. Vocabulary learned from training data. Tokenizer converts text $\rightarrow$ token IDs $\rightarrow$ model processes $\rightarrow$ token IDs $\rightarrow$ detokenizer converts to text.}\\
\circuitimpl{Input/output preprocessing. Affects what model can represent and process. Character-level vs. word-level vs. subword tradeoffs.}
\end{implementationbox}

\begin{ablationbox}
\textbf{Expected ablation:} Cannot process text without tokenization. Model requires discrete token inputs. Poor tokenization degrades model capabilities (e.g., character-level for long sequences, word-level for rare words). Critical preprocessing step.
\end{ablationbox}

\begin{examplebox}
\exinput{Text: ``The cat's really fast''}\\
\exbehavior{Tokenize: [``The'', ``cat'', ``'s'', ``really'', ``fast''] (example split)}\\
\exeffect{Discrete sequence suitable for model processing}
\end{examplebox}

\mechfooter{\statuswell}{embedding-unembedding, vocabulary-coverage}

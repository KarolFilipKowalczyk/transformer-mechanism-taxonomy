%=============================================================================
% BIBLIOGRAPHY.BIB - References for Transformer Mechanisms Naming Convention
%=============================================================================

% Core Mechanistic Interpretability
@article{elhage2021mathematical,
    title={A Mathematical Framework for Transformer Circuits},
    author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and others},
    journal={Transformer Circuits Thread},
    year={2021},
    url={https://transformer-circuits.pub/2021/framework/index.html}
}

@article{olsson2022context,
    title={In-context Learning and Induction Heads},
    author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and others},
    journal={arXiv preprint arXiv:2209.11895},
    year={2022}
}

@article{wang2022interpretability,
    title={Interpretability in the Wild: A Circuit for Indirect Object Identification in GPT-2 Small},
    author={Wang, Kevin and Variengien, Alexandre and Conmy, Arthur and others},
    journal={arXiv preprint arXiv:2211.00593},
    year={2022}
}

% Transformer Architecture
@article{vaswani2017attention,
    title={Attention is All You Need},
    author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and others},
    journal={Advances in Neural Information Processing Systems},
    volume={30},
    year={2017}
}

% MLP as Key-Value Memory
@article{geva2021transformer,
    title={Transformer Feed-Forward Layers Are Key-Value Memories},
    author={Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
    journal={arXiv preprint arXiv:2012.14913},
    year={2021}
}

@article{geva2023dissecting,
    title={Dissecting Recall of Factual Associations in Auto-Regressive Language Models},
    author={Geva, Mor and Bastings, Jasmijn and Filippova, Katja and Globerson, Amir},
    journal={arXiv preprint arXiv:2304.14767},
    year={2023}
}

% Knowledge Neurons
@article{dai2022knowledge,
    title={Knowledge Neurons in Pretrained Transformers},
    author={Dai, Damai and Dong, Li and Hao, Yaru and Sui, Zhifang and Chang, Baobao and Wei, Furu},
    journal={arXiv preprint arXiv:2104.08696},
    year={2022}
}

@article{meng2022locating,
    title={Locating and Editing Factual Associations in GPT},
    author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
    journal={arXiv preprint arXiv:2202.05262},
    year={2022}
}

% Sparse Autoencoders and Superposition
@article{elhage2022toy,
    title={Toy Models of Superposition},
    author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and others},
    journal={Transformer Circuits Thread},
    year={2022},
    url={https://transformer-circuits.pub/2022/toy_model/index.html}
}

@article{bricken2023towards,
    title={Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
    author={Bricken, Trenton and Templeton, Adly and Batson, Joshua and others},
    journal={Transformer Circuits Thread},
    year={2023},
    url={https://transformer-circuits.pub/2023/monosemantic-features/index.html}
}

@article{cunningham2023sparse,
    title={Sparse Autoencoders Find Highly Interpretable Features in Language Models},
    author={Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and others},
    journal={arXiv preprint arXiv:2309.08600},
    year={2023}
}

% Safety and Refusal
@article{arditi2024refusal,
    title={Refusal in LLMs is Mediated by a Single Direction},
    author={Arditi, Andy and Obeso, Oscar and others},
    journal={arXiv preprint arXiv:2406.11717},
    year={2024}
}

@article{zhou2025refusal,
    title={Refusal Falls off a Cliff: How Safety Alignment Fails in Reasoning},
    author={Zhou, Andy and others},
    journal={arXiv preprint},
    year={2025}
}

% Model Surveys
@article{zheng2025attention,
    title={Attention Heads of Large Language Models: A Survey},
    author={Zheng, Zifan and Wang, Yezhaohui and others},
    journal={Patterns},
    year={2025}
}

@article{rai2024practical,
    title={A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models},
    author={Rai, Daking and Lee, Yilun and others},
    journal={arXiv preprint arXiv:2407.02646},
    year={2024}
}

% Language Models
@article{brown2020language,
    title={Language Models are Few-Shot Learners},
    author={Brown, Tom and Mann, Benjamin and others},
    journal={Advances in Neural Information Processing Systems},
    year={2020}
}

@article{touvron2023llama,
    title={LLaMA: Open and Efficient Foundation Language Models},
    author={Touvron, Hugo and Lavril, Thibaut and others},
    journal={arXiv preprint arXiv:2302.13971},
    year={2023}
}

@article{achiam2023gpt,
    title={GPT-4 Technical Report},
    author={Achiam, Josh and Adler, Steven and others},
    journal={arXiv preprint arXiv:2303.08774},
    year={2023}
}

% Training and Alignment
@article{ouyang2022training,
    title={Training Language Models to Follow Instructions with Human Feedback},
    author={Ouyang, Long and Wu, Jeffrey and others},
    journal={arXiv preprint arXiv:2203.02155},
    year={2022}
}

@article{bai2022constitutional,
    title={Constitutional AI: Harmlessness from AI Feedback},
    author={Bai, Yuntao and Kadavath, Saurav and others},
    journal={arXiv preprint arXiv:2212.08073},
    year={2022}
}

% Additional Interpretability
@article{voita2019analyzing,
    title={Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned},
    author={Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
    journal={arXiv preprint arXiv:1905.09418},
    year={2019}
}

@article{bills2023language,
    title={Language Models Can Explain Neurons in Language Models},
    author={Bills, Steven and Cammarata, Nick and Mossing, Dan and others},
    journal={OpenAI Blog},
    year={2023}
}

@misc{nostalgebraist2020interpreting,
    title={Interpreting GPT: The Logit Lens},
    author={nostalgebraist},
    year={2020},
    url={https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens}
}

% Independence Criteria and Methodology
@article{geiger2021causal,
    title={Causal Abstractions of Neural Networks},
    author={Geiger, Atticus and Lu, Hanson and Icard, Thomas and Potts, Christopher},
    journal={Advances in Neural Information Processing Systems},
    volume={34},
    year={2021}
}

@article{chan2022causal,
    title={Causal Scrubbing: A Method for Rigorously Testing Interpretability Hypotheses},
    author={Chan, Lawrence and Garriga-Alonso, Adri{\`a} and Goldowsky-Dill, Nicholas and others},
    journal={Alignment Forum},
    year={2022},
    url={https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing}
}

